///
The Problem
On the internet, graphic material such as maps, photographs, and charts that represent numerical information, are clear and straightforward to those who can see it. For people with low vision, this is not the case. Rendering of graphical information is often limited to manually generated alt-text HTML labels, often abridged, and lacking in richness. This represents a better-than-nothing solution, but remains woefully inadequate. Artificial intelligence (AI) technology can improve the situation, but existing solutions are non-interactive, and provide a minimal summary at best, without offering a cognitive understanding of the content, such as points of interest within a map, or the relationship between elements of a schematic diagram. So, the essential information described by the graphic frequently remains inaccessible.

Our Approach
We use rich audio (sonification) together with the sense of touch (haptics) to provide a faster and more nuanced experience of graphics on the web. For example, by using spatial audio, where the user experiences the sound moving around them through their headphones, information about the spatial relationships between various objects in the scene can be quickly conveyed without reading long descriptions. In addition, rather than only passive experiences of listening to audio, an optional haptic device can help the user literally feel aspects like regions of a landscape, objects found in a photo, or the trend of a line on a graph. This will permit interpretation of maps, charts, and photographs, in which the visual experience is replaced with multimodal sensory feedback, rendered in a manner that helps overcome access barriers for users who are blind, deafblind, or partially sighted.

///

Welcome, IMAGE Contributor!
An overall description of IMAGE can be found at IMAGE Project Public Website. This page is for those joining the project as contributors.

McGill team members: Please keep in mind that the IMAGE code repos are public, so make sure to author content and comments appropriately, and do not include any internal links or resources, including passwords or account information.

Project Board
The center point for working on IMAGE is our IMAGE Project Board, where we track all our work items and issues across multiple repositories. Guidelines:

Make issues specific and measurable so it is easy to determine whether it is complete (e.g., "work on x" can linger forever)
Never move an issue left in the project board without discussing with your lead and adding an @jeffbl comment
Scope issues to no more than 3 person-days of work OR add more granular checklist in description
We essentially use the Github flow tutorial model for development, more detail in the Github docs on pull requests/branching.
Development Resources
SRL repos beginning with IMAGE-* are project repos. Each should be described on its homepage. Public ones are:
Server components: https://github.com/Shared-Reality-Lab/IMAGE-server
Browser extension: https://github.com/Shared-Reality-Lab/IMAGE-browser
Graphics used for testing and associated test scripts: https://github.com/Shared-Reality-Lab/IMAGE-test-graphics
Wiki pages (default in the server repo, if not repo-specific) cover things like the test/production servers, docker usage, etc.
McGill Internal Development
If you're on the McGill development team, we have some additional internal resources you'll need to sign up for. Ask @jeffbl for info.

For general SRL tools, e.g., group calendar, lab policies, etc., see the SRL internal wiki. Note that you will need to use the McGill VPN to get in.
We use Slack for quick communication, meeting reminders, etc. but please don't use it as a substitute for github issues.
In the Slack #general channel, there are pinned messages pointing to:
Google Drive, containing internal documentation related to funding, user studies, etc. Some documents you may want to scan when you first join:
StatementOfWork: what we've promised to funders
Open Source Audit: Spreadsheet of open source components we use. Must be kept up-to-date, since this creates licensing and acknowledgement obligations.
Orgchart: An overview of how everyone fits together on the project
Times and online links to weekly meetings
Zotero library of relevant papers and articles
UX resources
Anything user testing and/or participant-related such as testing procedures, consent forms, and REBs is STRICTLY controlled as per McGill confidentiality practices. Some resources such may be found in the UX repo or be found in the UX repo wiki. For anything else, contact @Cybernide for access.

///

Every person working on IMAGE doesn't have to be an expert at all of the below tools, but at least some familiarity with all of them will make the overall project make more sense. If you find better tutorials or resources, please add them!

Using unicorn for testing, you'll be connecting using SSH. This is so important that we have a whole wiki page on it!
Once logged in, you'll be using the Linux command line. You should be able to navigate the filesystem, chain commands together with | (pipe), understand file ownership and groups, and be familiar with basic commands like cd , ls, ls -l, cat, less, cp, mv, grep, rm, ping, and an editor like nano or any other of your choice. More advanced commands of likely use include tmux, glances, watch, grep regular expressions.
Git, and github, especially the github flow. You should be able to clone a repo, make a branch, make changes, merge main back into that branch if main has changed, and create a pull request (PR) that is linked to a work item. If you need help, you'll find that we expect that you're using the command line version of git for basic operations, or github's UI for things like PRs. If you're using a git GUI, that is fine, but most of us are not familiar with those tools. Sometimes these other tools do things in weird ways and can get you into a funny state, for which help may be limited since it isn't always obvious what happened.
Docker including docker-compose. If you're working on preprocessors or handlers, you should be able to write a Dockerfile, build an image, run that image as a container, and create a docker-compose.yml that has all the necessary Docker services, volumes, and networks. In most cases, you'll be able to use the existing docker-compose.yml as a reference.
Examples of public video introductions to Docker are available here and here.
An introduction to Docker in the context of the Shared Reality Lab is available on the lab wiki along with other useful materials.
Some more video tutorials to Docker showing how to integrate with the command line here and here
Basic networking and HTTP, since all our containers use http for communication. Note in particular the HTTP error codes which are used to report status between containers.
Another simple guide to HTTP basics
Windows users who have trouble using SSH and cannot git clone repos on other machines can follow these instructions: SSH for connecting to servers and to github

///

Practical notes on server use
We're using pegasus.cim.mcgill.ca for our production server. A second machine, unicorn.cim.mcgill.ca, is available for testing. Note that other projects are using these servers as well, so we have to be good citizens.

General use
Jeff or Juliette can create you an account on unicorn. You'll need to be in the iamge group to access the /var/docker/image directory, which contains the persistent content for our docker containers, including the website.
There are channels for #unicorn and #pegasus in the SRL slack. If you care about what happens on these machines (reboots, storage issues, etc.) make sure you are subscribed.
Jaydeep and Juliette (and Jeff as a fallback) are the unicorn/pegasus (and therefore docker) point people for the IMAGE team. Contact one of them if anything is unclear. Also make them aware if you have any issues that would impact server/docker status, since unless you've told them, they will happily let other groups do docker prunes or reboot the server without necessarily checking with the IMAGE team.
For machine learning, unicorn has two NVIDIA graphics cards. We cannot put these under heavy load on an ongoing basis without checking with other users. Normally, you bring up your containers to test, do you test, then shut them down to free resources for others.
GPUs aren't available to docker containers by default. Be sure to read the docs if you need them.
There is currently no automatic backup of these servers, so make sure you have everything saved safely elsewhere in case the entire server fails. Normally, this should mean pulling from git to update the server.
ML Models for IMAGE
Do not commit ML models to the git repositories. Git suffers with large, binary files and with large repositories, and so ML models pose a problem.
If you are using a model that is publicly available elsewhere:
In Docker, download the model into your image using something like wget. Note that this means the download should happen in the Dockerfile, not at runtime!
Clearly document where you got the model from. Ideally this would be a repository/archive with the software plus a paper on the model if one exists, but if not anything necessary so another person can understand what is happening.
If you are making your own model:
Clearly document the dataset used to train and the method you followed. It should be possible for anyone to recreate your model.
Store the model file on pegasus under /var/docker/atp/models and download the file into your Docker image from https://pegasus.cim.mcgill.ca/atp/models/[YOUR MODEL].
Docker
IMAGE uses many docker containers working together to take requests from the browser extension and return renderings. Please read this section carefully before working with Docker on any of the SRL servers.

Howto
If you need to work with Docker, you'll need to be a member of the docker group. Log out and back in for this change to take effect.
There are many docker tutorials online. If you haven't used docker before, please do some self-study to get the basics. The official docker documentation is good and up to date.
Be careful: With great power comes great responsibility. There is no protection between docker containers or images. If you run the wrong command (be especially careful with commands like "docker prune") you can easily damage other projects.
Requirements you MUST follow
Pegasus is for production: It is not a place to experiment with docker. You can run docker on your personal machine to learn how to use it, or use unicorn for testing.
Naming images and containers: All images and containers must be named in a way that makes it clear what they are, and what project they are used by. Default randomly assigned names are not allowed.
No pruning: Do not run "prune" even if you think it is super safe. Instead ask Jeff or Juliette to do it. In particular, do NOT run 'docker container prune', since other teams have data inside non-running containers, and they may not be easy (or even possible) to re-create. Members of the IMAGE project are not allowed to put non-static data inside containers (see next point), but we have to avoid causing problems with other projects that have done this.
Containers must be disposable: Do not create any data or configuration inside a running container. Map a volume instead. If you are running 'docker exec -it' for anything except debugging or seeing status, you're likely breaking this rule. Put another way: Docker containers are not virtual machines.
Volumes should be backed up: While volumes are persistent and not deleted with typical pruning, they're still fragile. Volume data can be deleted or overwritten by anyone in the docker group and is less safe than a file in your home directory. Anything that needs to be saved for a long period of time or can't be easily recreated should have a copy outside of docker altogether and possibly outside of pegasus.
When launching containers manually, make sure to use the --rm flag to remove the container automatically when it exits. Otherwise we end up with many lingering containers cluttering the system.
Helpful notes
The "glances" command will show you an overview of all the running containers and lets you watch as they are created and destroyed.
If you're temporarily using a container, docker run --rm will automatically destroy the container when it is stopped, and prevent docker container clutter.
Using compose is a good way to keep track of containers. It lets you set up multi-container environments in an easy, reproducible way, and gives sensible names by default. It is highly recommended for anything that won't be immediately deleted.
You may have TLS issues when using git and a remote repository over https. If this happens, using git over SSH should sidestep the problem. Version control is important!
If you're building images on unicorn, tag them with a version or date. When a new image is built with the same tag, the other becomes unlisted and is more difficult to remove. Also avoid tagging images you build locally as "latest" for the same reason.
Unresolved issues and Future plans
Right now, pegasus and unicorn have a traefik reverse proxy running (in docker, of course). /var/docker/traefik contains this configuration, but you should not need to edit it. If you do, please contact Juliette.
Every night, a cron job runs:'docker image prune' and docker container prune on both unicorn and pegasus. Do not keep state inside your containers!

///

Handlers, Preprocessors and Services: What are they?
The server architecture is not monolithic. Instead, it is built up of various components running as Docker services (in swarm mode) or Docker containers (regularly). These components fill various roles in the process of creating renderings to be sent to the user, but they all communicate with each other using HTTP* and are stateless. They should also all follow typical coding guidelines for the language they are written in, for example PEP 8 for Python. More information about the architecture can be found in our W4A'22 communication paper.

*The SuperCollider service uses OSC instead, but this is an exception as SuperCollider does not have HTTP support.

Preprocessors
Preprocessors are components that add additional data to a request sent by the client. This data can be as simple as checking if the URL of the page matches some pattern (e.g., is it from CNN?) or as complicated as performing some ML process on the image. Whatever a preprocessor does it MUST:

Listen for POST requests at the /preprocessor route where the body is the previously mentioned request JSON.
Reply to these POST requests with a properly-formatted response in the event the preprocessor action succeeds.
If the request is improperly formatted, the preprocessor must respond with a 400 status and SHOULD report the error in the body of the response.
The preprocessor should also check if the response generated passes validation. If it performs this check and the response fails, it must respond with a 500 status indicating the issue occurred in the preprocessor.
Finally, there are cases where everything is correct, but the preprocessor shouldn't run. In this case, a 204 status should be returned with no body. Cases like this are elaborated on below.
The response must be generated and sent within 15 seconds as of 2021-05-25. If a response is not received in this time, the connection is broken and the preprocessor data will not be available later on.
Handlers
Handlers are component that, using the preprocessor data and the request itself, generate renderings that can be selected by the user. These renderings are accompanied by a confidence score (0–100%) indicating how likely it is that the rendering correctly conveys information on the source image. 0% indicates that the handler is absolutely certain any rendering is incorrect, while 100% should be reserved for manually created renderings for certain media (e.g., a handler that displays a custom map of the metro network and nothing else). Handlers have similar requirements to preprocessors in that they must:

Respond to POST requests at /handler containing the request JSON with a well-formatted response.
If the request JSON is not properly formatted, it must respond with a 400 error.
If for any other reason the handler cannot generate a proper response (i.e., a failure occurs), it must respond with a 500 error.
If the handler can't actually generate anything, it should respond with an empty list of renderings.
The handler should respond as quickly as possible despite not having a fixed timeout period. It is likely that at some point handlers that do not respond quickly will be cancelled. Since many handlers will run in parallel, resource contention will be an issue. So, it may be most efficient to emphasize quickly deciding that the handler cannot produce a good rendering, and terminate as early as possible, rather than generating an entire rendering that will have a low confidence score.
Services
Services perform fixed convenience jobs to multiple preprocessors and handlers. A service should essentially do one kind of thing well, like TTS, that is likely to be reused. Services are less strictly defined than preprocessors and handlers, but still should follow some guidelines:

For ease of use, they should respond to requests on port 80.
Each service should define the requests and responses it deals with.
If possible, these should be shared across a type of service to permit easily switching (e.g., all TTS services use a shared interface).
The route for requests should be /service/[generic type]/[specific command]. E.g., creating a set of TTS audio uses /service/tts/segment-tts.
Docker Compose Configuration
This section applies to preprocessors and handlers. Services also run in Docker, but do not need this configuration as they don't need to be advertised to a central service. The preprocessors and handlers need to be visible to a receiver component that actually receives the request from the client, passes this to other components in order, and enforces schema compliance.

To find these other components, the receiver connects to the Docker daemon and looks for services/containers with certain labels. For preprocessors, the preprocessor must contain the label ca.mcgill.a11y.image.preprocessor where its value is a number. This number is used to sort the preprocessors. So all preprocessors with this label set to 1 will run before those set to 2. This is used to allow preprocessors to ensure access to information from other preprocessor processes. For handlers, this label is ca.mcgill.a11y.image.handler which MUST be set to the string "enable". Handlers run in parallel, and so no ordering is required.

Both handlers and preprocessors are assumed to be running on port 80 by default. If they do not use this port, then they must specify the port using the ca.mcgill.a11y.image.port label. So, a preprocessor that relies on data from another preprocessor and listens to port 8080 would have the following labels fragment in a Docker compose file:

labels:
    ca.mcgill.a11y.image.preprocessor: 2
    ca.mcgill.a11y.image.port: 8080
Preprocessor Chaining Best Practices
As explained on above, preprocessors run in an order and must return 204 quickly if they cannot do anything with a request. While this conclusion can sometimes be made from the original request (e.g., this preprocessor only works with images from a certain site), often this requires looking at the output of a previously run preprocessor. It is a goal of IMAGE to keep preprocessors (and other components) modular so they can be reused and repurposed on another server instance. This other instance may not have the same goals or requirements as our instance. To avoid someone needing to edit our code to adapt to a particular use case, we require the following to be done for preprocessors that use the output of another preprocessor:

If the previous preprocessor's output is available and indicates conditions where the current preprocessor should run, it should run normally.
If the previous preprocessor's output is available and indicates conditions where the current preprocessor should not run, it should return 204.
If the previous preprocessor's output is not available and its data is not strictly necessary for the current preprocessor to run, it should run normally.
If the previous preprocessor's output is not available and its data is strictly necessary for the current preprocessor to run, it should return 204.
In no case should a preprocessor return a 400 or 500 error when a request was properly formatted and no operational errors had occurred.

///

3. Creating, testing, and deploying preprocessors and handlers
 
Antoine Phan edited this page on Jun 6 · 32 revisions
 Pages 12
Find a page…
Home
0. Background technology for working on IMAGE
1. Servers and Docker
2. Handlers, Preprocessors and Services
3. Creating, testing, and deploying preprocessors and handlers
Making a preprocessor
Cleanup
Notes on unicorn / Tips and Tricks
Production (pegasus)
Guidelines for logging and error handling
FAQ
4. SSH for connecting to servers and to github
5. How we use github
6. IMAGE Terminology
7. Multilang Support
Licensing and Copyright Guidelines
Literature Survey for charts
Supercollider Flowcharts
 Add a custom sidebar
Clone this wiki locally
https://github.com/Shared-Reality-Lab/IMAGE-server.wiki.git
Although other wiki pages cover what preprocessors and handlers are, and general rules concerning docker and server use, this page directly discusses the path from getting code working on your local machine, through testing, and into production. Much of the example text may be McGill-specific, but the general approach is likely valid for anyone creating new preprocessors or handlers.

Juliette created video tutorials that explain server use and preprocessor creation and handler creation and testing. They give a more hands-on walkthrough with examples.

IMPORTANT: To use github from unicorn, you'll need to set up SSH ForwardAgent. See the SSH page! Do not skip this step. If you are doing a live onboarding session, you need to have the repository cloned in your homedir on unicorn BEFORE the start of the onboarding session.

Making a preprocessor
Note: Much of this can be done in docker on your local machine. This would let you create and test locally. For the purposes of illustration, however, we'll be doing everything directly on unicorn. If you want to develop on your local machine, we primarily work with docker on Linux, but some have had success with WSL: Docker for windows. YMMV.

If you are starting from scratch, probably best to start with a minimal template unless there is another that is closer to what you're building: Preprocessor minimal example. We're walking through a preprocessor, but creating/modifying a handler should be similar: Handler minimal example. We'll assume that we're going to modify the hello-preprocessor, so we'll clone and make a new branch. NOTE: We use the name test_preprocessor throughout this tutorial. We recommend, especially if you are participating in a group onboarding with others doing the tutorial at the same time, to please use your userid as a prefix to anything you're creating (e.g., USERID_test_preprocessor) so that we can tell them all apart!

git clone --recurse-submodules git@github.com:Shared-Reality-Lab/IMAGE-server.git
cd IMAGE-server/preprocessors/hello-preprocessor
git checkout -b test_preprocessor
Note the Dockerfile that creates an image capable of running your code:

jeffbl@unicorn ~/t/I/p/hello-preprocessor (test_preprocessor)> cat Dockerfile 
FROM node:alpine

WORKDIR /usr/src/app

# Apparently splittig this up is good for layers
# Docker images are onions
COPY /preprocessors/hello-preprocessor/package*.json ./
RUN npm ci
COPY /preprocessors/hello-preprocessor/ .
COPY /schemas src/schemas
RUN npm run build

ENV NODE_ENV=production

EXPOSE 8080

USER node
CMD [ "node", "dist/server.js" ]
To ensure that data is passed in the proper format without errors, IMAGE relies heavily on JSON schemas, which are a formal statement of exactly what the JSON can contain. These are included using the COPY /schemas src/schemas line of the Dockerfile. If you are building a container manually, this assumes you are doing so from the root of the server repo:

juliette@unicorn:~/Documents/IMAGE-server$ pwd
/home/juliette/Documents/IMAGE-server
juliette@unicorn:~/Documents/IMAGE-server$ docker build -t hello-preprocessor:test -f preprocessors/hello-preprocessor/Dockerfile .
Sending build context to Docker daemon  123.7MB
Step 1/11 : FROM node:alpine
 ---> 789fb8adc830
Step 2/11 : WORKDIR /usr/src/app
 ---> Using cache
 ---> 7da4734d6300
Step 3/11 : COPY /preprocessors/hello-preprocessor/package*.json ./
 ---> a011305c9fb1
Step 4/11 : RUN npm ci
 ---> Running in fc6f331d2750

added 203 packages, and audited 204 packages in 2s

25 packages are looking for funding
  run `npm fund` for details

found 0 vulnerabilities
npm notice 
npm notice New patch version of npm available! 8.19.1 -> 8.19.2
npm notice Changelog: <https://github.com/npm/cli/releases/tag/v8.19.2>
npm notice Run `npm install -g npm@8.19.2` to update!
npm notice 
Removing intermediate container fc6f331d2750
 ---> bb25548c0238
Step 5/11 : COPY /preprocessors/hello-preprocessor/ .
 ---> 12dc12f6a22b
Step 6/11 : COPY /schemas src/schemas
 ---> 3a64f2e5737e
Step 7/11 : RUN npm run build
 ---> Running in c8bf6d503dd2

> hello-preprocessor@0.0.1 prebuild
> eslint --ext .ts src


> hello-preprocessor@0.0.1 build
> tsc

Removing intermediate container c8bf6d503dd2
 ---> fa687259e27e
Step 8/11 : ENV NODE_ENV=production
 ---> Running in 81b80d381c4c
Removing intermediate container 81b80d381c4c
 ---> fd35e40b6c72
Step 9/11 : EXPOSE 8080
 ---> Running in bb188a1d988e
Removing intermediate container bb188a1d988e
 ---> c3c1a795bdab
Step 10/11 : USER node
 ---> Running in d7d80588000a
Removing intermediate container d7d80588000a
 ---> c36a33fa7f15
Step 11/11 : CMD [ "node", "dist/server.js" ]
 ---> Running in 5ec918f6e4e7
Removing intermediate container 5ec918f6e4e7
 ---> c293f00f6668
Successfully built c293f00f6668
Successfully tagged hello-preprocessor:test
juliette@unicorn:~/Documents/IMAGE-server$
Normally, unicorn is running all of the unstable built docker images for the project via /var/docker/image/docker-compose.yml, as merged into the main branch. General information can be found in the docker-compose documentation. When you want to run additional or different preprocessors (or handlers, or services!) before they get merged you'll use a docker-compose.override.yml in the same directory. This docker-compose.override.yml can also be used to build the container you've modified:

services:
  test-preprocessor:
    image: "test-preprocessor:test"
    build:
      context: /home/juliette/Documents/IMAGE-server
      dockerfile: /home/juliette/Documents/IMAGE-server/preprocessors/hello-preprocessor/Dockerfile
    env_file:
      ./auditory-haptic-graphics-server/config/express-common.env # This is necessary to just accept larger requests when using express!
NB: the docker-compose.override.yml file may already be used by someone else! If that's the case, add your changes to theirs as new keys in YAML rather than deleting what's already there. Then, everyone's overrides will be reflected. However if someone is modifying the same key (e.g., you're both working on test-preprocessor) then you will need to coordinate with them.

You can build your image and start the container at the same time by running docker compose up -d test-preprocessor. Note that this will only rebuild your image if one does not already exist. If you need to rebuild an existing image, then you would run docker compose up --build -d test-preprocessor.

juliette@unicorn:/var/docker/image$ docker compose up -d test-preprocessor
Building test-preprocessor
Sending build context to Docker daemon  123.7MB
Step 1/11 : FROM node:alpine
 ---> 789fb8adc830
Step 2/11 : WORKDIR /usr/src/app
 ---> Using cache
 ---> 8d7ddc221959
Step 3/11 : COPY /preprocessors/hello-preprocessor/package*.json ./
 ---> Using cache
 ---> feff94c72892
Step 4/11 : RUN npm ci
 ---> Using cache
 ---> 142b90f90d4d
Step 5/11 : COPY /preprocessors/hello-preprocessor/ .
 ---> Using cache
 ---> cf59c094335a
Step 6/11 : COPY /schemas src/schemas
 ---> Using cache
 ---> bf3ddaf3a157
Step 7/11 : RUN npm run build
 ---> Using cache
 ---> 1056f83e9508
Step 8/11 : ENV NODE_ENV=production
 ---> Using cache
 ---> ae6691674b41
Step 9/11 : EXPOSE 8080
 ---> Using cache
 ---> cb638be7ae65
Step 10/11 : USER node
 ---> Using cache
 ---> 0ed10feb4440
Step 11/11 : CMD [ "node", "dist/server.js" ]
 ---> Using cache
 ---> cf991078d800
Successfully built cf991078d800
Successfully tagged test-preprocessor:test
WARNING: Image for service test-preprocessor was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker compose up --build`.
Creating image-test-preprocessor-1 ... done
juliette@unicorn:/var/docker/image$ 
Now the test-preprocessor image is running as a container called image-test_preprocessor-1 on unicorn with an internal IP address. We can determine this IP address using the docker inspect command:

juliette@unicorn:/var/docker/image$ docker inspect image-test-preprocessor-1 | grep IPAddress
            "SecondaryIPAddresses": null,vim 
            "IPAddress": "",
                    "IPAddress": "192.168.240.25",
juliette@unicorn:/var/docker/image$
Now we can see what the preprocessor container outputs directly. We'll use helper scripts for IMAGE to do this located in /var/docker/image/bin. The make_request script takes a graphic file and generates an appropriate request. The sendimagereq script sends a request to a URL. Note that our test-preprocessor container runs by default on port 8080.

juliette@unicorn:/var/docker/image$ pwd
/var/docker/image
juliette@unicorn:/var/docker/image$ /var/docker/image/bin/make_request /home/juliette/London_Street_1_920_690_80.jpg | /var/docker/image/bin/sendimagereq - http://192.168.240.25:8080/preprocessor
{"request_uuid":"f24326b3-6250-4666-91aa-903576e8b52f","timestamp":1663176228,"name":"ca.mcgill.a11y.image.hello.preprocessor","data":{"message":"Hello, World!"}}juliette@unicorn:/var/docker/image$ 
juliette@unicorn:/var/docker/image$ /var/docker/image/bin/make_request /home/juliette/London_Street_1_920_690_80.jpg | /var/docker/image/bin/sendimagereq - http://192.168.240.25:8080/preprocessor | jq
{
  "request_uuid": "26a49bc7-8c3e-41dd-ad25-36bbd41eb854",
  "timestamp": 1663176454,
  "name": "ca.mcgill.a11y.image.hello.preprocessor",
  "data": {
    "message": "Hello, World!"
  }
}
Here we have the raw output from the preprocessor, including a request_uuid, a timestamp, a name indicating that this is the "hello preprocessor" (you would change this for anything that would get into production!), and a data field. The data field contains the outputs of the preprocessor, in this case a message with hello world. In the above example, we use the jq utility to pretty print the output. This is a very powerful tool for manipulating JSON files, but you likely would need to install it manually on your own computer.

In practice, preprocessors aren't called like this, they're made available to the orchestrator using different docker labels. Then the preprocessor will be called along with all other preprocessors when a request is received. First we modify the docker-compose.override.yml file to add these labels. Note that if you're modifying an existing preprocessor, these labels likely exist in the docker-compose.yml file and do not need to be copied.

services:
  test-preprocessor:
    image: "test-preprocessor:test"
    build:
      context: /home/juliette/Documents/IMAGE-server
      dockerfile: /home/juliette/Documents/IMAGE-server/preprocessors/hello-preprocessor/Dockerfile
    env_file:
      - ./auditory-haptic-graphics-server/config/express-common.env
    labels:
      ca.mcgill.a11y.image.preprocessor: 1
      ca.mcgill.a11y.image.port: 8080
For more information about these labels, see the section on docker compose configuration for handlers, preprocessors, and services.

Then we can send the same request we sent to our test-preprocessor to the orchestrator. To do this, we need to get its IP address, send the request, and filter the output using jq to only see the section related to the test-preprocessor. Note that the orchestrator runs on port 8080.

juliette@unicorn:/var/docker/image$ pwd
/var/docker/image
juliette@unicorn:/var/docker/image$ docker inspect image-orchestrator-1 | grep IPAddress
            "SecondaryIPAddresses": null,
            "IPAddress": "",
                    "IPAddress": "192.168.240.14",
juliette@unicorn:/var/docker/image$ docker compose up -d test-preprocessor # apply the new labels
Recreating image-test-preprocessor-1 ... done
juliette@unicorn:/var/docker/image$ /var/docker/image/bin/make_request /home/juliette/London_Street_1_920_690_80.jpg | /var/docker/image/bin/sendimagereq - http://192.168.240.14:8080/render/preprocess | jq '.preprocessors."ca.mcgill.a11y.image.hello.preprocessor"' -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  851k  100  686k  100  164k   250k  61238  0:00:02  0:00:02 --:--:--  309k
{
  "message": "Hello, World!"
}
juliette@unicorn:/var/docker/image$ 
The content of data is now available in the output of the preprocessors-only call to the orchestrator (/render/preprocess) under the keys preprocessors and then ca.mcgill.a11y.image.hello.preprocessor, the name that was shown earlier. Each preprocessor being run at once should have a unique name to avoid conflicts.

But why use this docker-compose.override.yml? The /var/docker/image/docker-compose.yml file sets up our test server at https://unicorn.cim.mcgill.ca/image. This means that everything above can actually be condensed down to the following:

juliette@unicorn:/var/docker/image$ pwd
/var/docker/image
juliette@unicorn:/var/docker/image$ /var/docker/image/bin/make_request /home/juliette/London_Street_1_920_690_80.jpg | /var/docker/image/bin/sendimagereq - https://unicorn.cim.mcgill.ca/image/render/preprocess | jq '.preprocessors."ca.mcgill.a11y.image.hello.preprocessor"' -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  851k  100  686k  100  164k   243k  59589  0:00:02  0:00:02 --:--:--  301k
{
  "message": "Hello, World!"
}
juliette@unicorn:/var/docker/image$ 
The test-preprocessor is now running in a public way! It also would have access to all the other preprocessors running on unicorn at the same time and would be sent on to the handlers if a request was sent to /render rather than /render/preprocess. However, it is still running the original code. We can modify its output by tweaking its source files back in the repository. For this example, we edit preprocessors/hello-preprocessor/src/server.ts and change these lines:

app.post("/preprocessor", (req, res) => {
    if (ajv.validate("https://image.a11y.mcgill.ca/request.schema.json", req.body)) {
        console.debug("Request validated");
        const response = {
            "request_uuid": req.body.request_uuid,
            "timestamp": Math.round(Date.now() / 1000),
            "name": "ca.mcgill.a11y.image.hello.preprocessor",
            "data": {
                "message": "Hello, World!"
            }
        };
to these:

app.post("/preprocessor", (req, res) => {
    if (ajv.validate("https://image.a11y.mcgill.ca/request.schema.json", req.body)) {
        console.debug("Request validated");
        const response = {
            "request_uuid": req.body.request_uuid,
            "timestamp": Math.round(Date.now() / 1000),
            "name": "ca.mcgill.a11y.image.hello.preprocessor",
            "data": {
                "message": "My new example message!"
            }
        };
Note that we are changing the data that is reported by the preprocessor. This will ultimately be available under the key ca.mcgill.a11y.image.hello.preprocessor in the output available at /render/preprocess or to handlers. To see our changes reflected we need to rebuild the image and bring up our container again with the new image. Back in the location with our docker-compose.override.yml file:

juliette@unicorn:/var/docker/image$ pwd
/var/docker/image
juliette@unicorn:/var/docker/image$ docker compose up --build -d test-preprocessor
Building test-preprocessor
Sending build context to Docker daemon  123.7MB
Step 1/11 : FROM node:alpine
 ---> 789fb8adc830
Step 2/11 : WORKDIR /usr/src/app
 ---> Using cache
 ---> 8d7ddc221959
Step 3/11 : COPY /preprocessors/hello-preprocessor/package*.json ./
 ---> Using cache
 ---> feff94c72892
Step 4/11 : RUN npm ci
 ---> Using cache
 ---> 142b90f90d4d
Step 5/11 : COPY /preprocessors/hello-preprocessor/ .
 ---> Using cache
 ---> cf59c094335a
Step 6/11 : COPY /schemas src/schemas
 ---> Using cache
 ---> bf3ddaf3a157
Step 7/11 : RUN npm run build
 ---> Using cache
 ---> 1056f83e9508
Step 8/11 : ENV NODE_ENV=production
 ---> Using cache
 ---> ae6691674b41
Step 9/11 : EXPOSE 8080
 ---> Using cache
 ---> cb638be7ae65
Step 10/11 : USER node
 ---> Using cache
 ---> 0ed10feb4440
Step 11/11 : CMD [ "node", "dist/server.js" ]
 ---> Using cache
 ---> cf991078d800
Successfully built cf991078d800
Successfully tagged test-preprocessor:test
Creating image-test-preprocessor-1 ... done
juliette@unicorn:/var/docker/image$ 
Rerunning the same commands as above will now show the updated message in the preprocessor. Outside of this example, you would make more significant modifications (or create an entirely new preprocessor). The commands you've already run would let you see the direct outputs of the preprocessor, and using the extension would allow you to see how the preprocessor's data is used in the handlers (if any are set up to use that data).

Build your image by running docker-compose build $SERVICE_NAME. In this example, docker-compose build test-preprocessor. Note that docker-compose build by default builds all components listed in docker-compose.yml.

If you're running locally, you won't have all of the unstable modules already running, as they are on unicorn, so you will need to also build any other images you need in order to test, such as the orchestrator.

Once the build is complete, run your container and any containers you need by calling docker compose up -d $LIST_OF_CONTAINERS. You could run the containers with docker run --rm (and must include --rm), but docker-compose will also set up any additional infrastructure needed for IMAGE to work. A list of container names will be output when started (e.g., auditory-haptic-graphics-server_orchestrator_1).

Determine the IP address of any container you need. On Linux, these addresses can be reached by the host and found using docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' $CONTAINER_NAME.

On macOS, these addresses are NOT reachable and any containers you need to access will need to have its port bound to the host. So to bind port 8080 of the orchestrator to port 8080 of the host, the section of docker-compose.yml will look as follows:

orchestrator:
  build:
    context: ./orchestrator
    dockerfile: Dockerfile
  image: "orchestrator:latest"
  env_file:
    - ./config/express-common.env
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock
  ports:
    - 8080:8080
This is only for testing on MacOS - these lines for adding port binding should not be committed to the repository!

Test if the docker container runs successfully by either using postman or curl to the endpoint above, e.g., curl -H "Content-Type: application/json" -d@request.json http://localhost:3001/preprocessor

To see what is going on, get all the logs from the docker containers with docker-compose logs -f

For a handler, since they run in parallel without dependencies between each-other, you can inject preprocessor output captured via the extension directly, and then check the output. For example, if you already have the preprocessor output in a file called preprocess.json and we want the outputs from a running container called image-photo-audio-handler, you could run:

juliette@unicorn:~$ docker inspect image-photo-audio-handler | grep IPAddress
            "SecondaryIPAddresses": null,
            "IPAddress": "",
                    "IPAddress": "192.168.240.12",
juliette@unicorn:~$ /var/docker/image/bin/sendimagereq preprocess.json http://192.168.240.12/handler | jq '.renderings | length' -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 1837k  100 1150k  100  686k   672k   401k  0:00:01  0:00:01 --:--:-- 1073k
2
juliette@unicorn:~$ /var/docker/image/bin/sendimagereq preprocess.json http://192.168.240.12/handler | jq '.renderings | map(.type_id)' -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 1826k  100 1140k  100  686k   665k   400k  0:00:01  0:00:01 --:--:-- 1065k
[
  "ca.mcgill.a11y.image.renderer.Text",
  "ca.mcgill.a11y.image.renderer.SegmentAudio"
]
juliette@unicorn:~$ 
This shows that there are two renderings being returned in this example: one that is plain text, one that is segmented audio. The full output is not shown here, however if you wanted to extract the MP3 file from the second SegmentAudio rendering, you could run the following:

juliette@unicorn:~$ /var/docker/image/bin/sendimagereq preprocess.json http://192.168.240.12/handler | jq -r '.renderings[1].data.audioFile' - | awk -F, '{ print $2 }' | base64 -d - > output.mp3
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 1844k  100 1157k  100  686k   672k   398k  0:00:01  0:00:01 --:--:-- 1071k
juliette@unicorn:~$ file output.mp3
output.mp3: MPEG ADTS, layer III, v1, 128 kbps, 48 kHz, JntStereo 
The command in jq moves into the second rendering and outputs the audioFile encoding as a data URL. The next awk command splits this at the , so only the base64 encoded part is left, not the prefix indicating the file type and encoding. The base64 command then decodes base64 to binary, and that binary file is written to output.mp3. The file command then gives us information on output.mp3, showing that it is in fact a valid MP3 file. By using variations of these commands, it's possible to test handlers in isolation and collect outputs from the command line.

Cleanup
When you are done testing, you need to clean up and restore unicorn to running a clean unstable. If you want to save your override, copy it into your homedir, then in /var/docker/image do a ./restoreunstable. This deletes any override files, rebuilds everything related to IMAGE, and clears out any extra running containers. You should now have a clean running server reflecting what is checked into main.

Note that if you want to clean everything up but still apply your override, you can instead use ./restoreunstable -k which will clean everything up and restore what is in main, but then also keep and apply any overrides.

Notes on unicorn / Tips and Tricks
Every morning around 5h00, unicorn is reset to be running the IMAGE system tagged unstable. This reflects what is checked into main. Any overrides will be deleted!

Please do not copy the entire docker-compose.yml file, name it docker-compose.override.yml, then make your changes within it. This has already caused issues, since if you use it again in the future, it will override pretty much everything in the base docker-compose.yml file. This will be impossible to debug and cause random behavior if the underlying docker-compose.yml is updated, since your overrides will create a huge mess. Don't do this!

If there is already a docker-compose.override.yml there when you need to test, simply edit it and add your changes, then remove them when you are done. Use ./restoreunstable -k to clean up when you're done, since you don't want to delete someone else's override file.

It is considered polite to message in the IMAGE slack #testing channel when you're going to be making changes on unicorn, and again when you're done, so that others who are testing know that things may be changing/broken while you're testing.

To test against with the browser extension, right click the IMAGE extension icon in yoru browser, and go to options. Make sure the server URL is https://unicorn.cim.mcgill.ca/image/

Make sure you use the same internal network, or the orchestrator won't pick up your container.

Do not run two preprocessors that produce output under the same JSON tags, since this will create a mess.

ADVANCED: You can also bring up your own entire stack of containers for testing, running in parallel with the unstable set. This may require making a new path in traefik if you want to run them at a different link (and independently of) the existing unstable set of containers. Contact Jaydeep or Juliette for help with this, but we'd prefer to avoid doing this unless really necessary...

docker ps -a will show you all running containers, including their container IDs. glances will monitor running containers, and is good to keep active to watch container status.

There are a number of useful tools in the /var/docker/image/bin/ folder on unicorn, including make_request, which creates a request for a graphic file that you can send for testing, and also gpu_memory_docker that shows you how much GPU memory each docker container is using.

Production (pegasus)
Putting features and fixes into production on pegasus is done by Jaydeep or Juliette. Eventually, we plan to release a new version of the extension and the latest server-side components at the end of each sprint, but for now it is more ad-hoc. For the browser extension, this is a manual process done by Jaydeep and Cyan. The process for deploying server-side components into production:

By default, each component in git is marked unstable by default when it is merged.
When a component is ready for deployment (including testing!) it should be tagged latest by the person who owns that component
Juliette or Jaydeep runs docker-compose pull && docker compose up -d on pegasus, which fetches and runs all of the components marked latest, and the latest versions are live!
Note: If you make a change that will break compatibility with the extension, you need to flag this to Juliette or Jaydeep so it can be managed. If you make another breaking change (e.g., modifying a preprocessor such that it will break other preprocessors or handlers), do not mark it latest until the other components have been updated and everything is tested together.

Guidelines for logging and error handling
Since the preprocessors, handlers, and services all work together, when unexpected behavior arises, it can be very difficult to quickly zero in on the root problem. To help with this we require every component to implement solid error checking, as well as message logging. When thinking about this, consider that your code will likely be deployed into production at some point. If something goes wrong, all you will get is the log file, with the information you output. If the system or a GPU runs out of memory, will you know what happened? If the graphic resizing library doesn't handle the type of graphic the user submits, will you know from just looking at the logs? If your component can't return a valid result, will you understand why? If your code hangs, where was it when it stopped? Remember that you may ONLY have the log file, and nothing else, when trying to figure out what happened! Some guidelines (HT Juliette for some of these from the onboarding presentation):

Know your log levels and use them

When your code moves from one section to another, log that!

If anything fails schema validation, log that!

If something occurs that means your expected output isn’t present, log that!

Make your log messages concise and identifiable

Be prepared for exceptions, especially with network requests & GPU tasks. Make sure to try/catch any code that makes significant library calls (e.g., graphic resizing), uses specific devices (e.g., GPUs) or that might take significant time to run (e.g., inferencing, loading a model). Before raising a PR, think through common failure points, and verify your code does the right thing in these cases.

Code runs on changing, shared environments over long periods. Assumptions will not hold.

Write code that fails gracefully, not code that runs perfectly

Preprocessors: logging output should clearly show the major events that occurred. Did it look at previous preprocessor output to decide whether or not to proceeed? Did it change the graphic in some way, like resizing? Did it get an unexpected result back from a library, e.g., when resizing the graphic? If it categorizes the graphic, is it obvious what categories it chose? These things should be obvious from the log!

Handlers: How does the hander decide whether it can produce a valid rendering? Are all of the services used returning expected results? Is there data it was expecting, but can't find? What parts of the handler take the most time to run?

IMPORTANT NOTE: For privacy reasons, do NOT log personal data from the query, including the URL of the graphic, or any other information specific to the content itself!

FAQ
ISSUE: What if I need >12GB of GPU memory, given unicorn's biggest card is only 12GB?

ANSWER: If you really need more than 12GB, that is likely to be problematic in production even though pegasus has a 24GB card. Nonetheless, if you can run a smaller version of the model, or move it to run on CPU (even though slow), you can likely still test on unicorn. If it is simply too big, contact Juliette or Jaydeep about testing (carefully) on pegasus, or other alternatives. If you're building a handler and don't need modifications to the current production preprocessors, get the preprocessor data from production using the Developer mode options available in extension settings, and test with that.

Thanks to Siddharth and Rohan for creating a draft set of instructions, which this incorporates.

///

For Linux / Mac
From a terminal, you should be able to ssh unicorn.cim.mcgill.ca to connect to unicorn. Set up SSH keys both for security and because it is far more convenient!

To use github while you are logged into unicorn, you will need to upload your public key to github through your github settings. However, DO NOT put your private key on unicorn so that you can git clone from github while on unicorn! The correct way is to use ForwardAgent in your ~/.ssh/config. This will safely forward your private key(s) through unicorn to github. For example, my ~/.ssh/config on my laptop contains:

 Host unicorn
        User <your unicorn account name>
 	HostName unicorn.cim.mcgill.ca
 	ForwardAgent yes
Then, from my laptop, I can ssh unicorn, then once I'm logged in, git clone --recurse-submodules git@github.com:Shared-Reality-Lab/IMAGE-server.git, and I don't have to type a password since my private key on my local computer is being forwarded to github for authentication. It might ask you to set your email address. Make sure to set it to one that is associated with your github account. At this point, you should be able to branch, make changes, and push to github, not passwords necessary!

For Windows
If you have a Windows machine and you are having trouble connecting to Unicorn or others via SSH fear not! This a fairly common problem and we will be addressing this issue in no time. Usually, this happens because Windows does not have the SSH-agent turned on by default which won't allow you to properly leverage the "Forwarding Agent" that will allow you to clone github repos on Unicorn.

Before we get started, make sure you have the most recent version of PowerShell installed and open a window as administrator. If you haven't already, you will need to generate an SSH-key and add it to your github account, you can follow instructions here This is important as it is this key that the "ForwardAgent" will use to access github.

IMPORTANT: DO NOT GENERATE A KEY PRIVATE KEY ON A MACHINE YOU DO NOT OWN.

Private keys, the ones generated by the ssh-keygen command, should only reside in a machine you have full control and cannot be reached by others for security reasons. You public keys, which is the one generated and saved on github is safe to use on other machines.

Finally by following the steps found in Practically using github while SSH'ed into a test or production server you should be able to access Unicorn and we are finally ready to set up our ForwardAgent.

Prior to running the ssh-agent, you will need to update OpenSSH as the default Windows ForwardAgent does not work with Ubuntu 22+. To do this you will open Powershell as administrator and run the following commands:

Search for the package:

winget search "openssh beta" Install new OpenSSH:
winget install "openssh beta"
Now the new OpenSSH should be installed!

Incase you want to uninstall you can run this command:

winget uninstall "openssh beta" 
Back to setting up the SSH ForwardAgent. If the follow command: Get-Service ssh-agent Gives you a "Stopped" status for your SSH agent as seen below:

Status   Name               DisplayName
------   ----               -----------
Stopped  ssh-agent          OpenSSH Authentication Agent
And if Get-Service ssh-agent | Select StartType Shows gives the following output:

StartType
---------
Disabled
This means that Windows does no currently have an agent running. To activate a new SSH-Agent you will need to do the following:

Set the agent to Manual activation:
Get-Service -Name ssh-agent | Set-Service -StartupType Manual

Then you activate your agent:
Start-Service ssh-agent

Add the path to your SSH-keys:
ssh-add path\to\ssh\key

Sanity check, if the following command gives you a long string of random things it worked! Otherwise it will give you a message regarding missing ssh-agents.
ssh-add -L

///

All code, and most documentation, should be in one of our github repositories. Please remember that our main repos are public!

Important:

NEVER check in an API/account key or password
NEVER check in a full machine learning model or other large file
NEVER check in binaries
Our use is based on github flow. Essentially, this means you will:

clone the repository
make a new branch
implement your changes
make sure you've synced from main so your branch is up-to-date
test your changes
create a pull request (PR)
assign the PR to the appropriate reviewer (ask if you are not sure who that is!)
if the reviewer assigns it back to you, make modifications, and assign PR back to reviewer
reviewer merges your branch to main
you delete the branch so it doesn't clutter up the repo
FAQ:

Can I just put the reviewer in the "Reviewer" field for my PR? ANSWER: No. Reviewers won't look at items that are not currently assigned to them.
Where should I put my big items like ML model files, if not in the git repo? ANSWER: We have a directory on the IMAGE Google drive that we copy to unicorn and pegasus.
Why Google Drive? Why not git annex, git LFS, etc.? ANSWER: We considered some of these, but they either had limitations, or we weren't sure how to set them up effectively for our use. If you're interested in taking this on, please speak up, since the Google Drive solution is not ideal for many reasons.
If I have documentation or instructions, can I just keep the information in the work item, then close it when I'm done? ANSWER: No, closed items are hard to find. Any ongoing documentation for a specific component should be in the README.md for the narrowest area possible. For example, research relating to machine learning models used in a preprocessor should be in the README.md for that specific preprocessor.
Should I fork any of our repos? ANSWER: No, you should clone.

///

IMAGE Terminology
There are a lot of terms used as part of IMAGE, whether in documentation, source code, or user interfaces. To avoid confusion, these terms (should) have specific meanings when used in the context of IMAGE.

Visual Media-related Terms
IMAGE - This is the Internet Multimodal Access to Graphical Exploration or IMAGE project. IMAGE contains various software and services, which are discussed on this wiki and on the main website. To avoid confusion, "image" (lowercase) should be avoided in documentation or user interfaces.
Graphic - This refers to any kind of visual media in a digital format. It indicates nothing about the content of the media (e.g., if it's a chart, graph, photograph).
IMAGE Architecture Terms
Orchestrator - The container that receives requests from the client. These requests contain a graphic or other data representing a visual element that will be used to generate non-visual renderings.
Preprocessor - A container that analyzes a request to extract or identify additional useful data.
Handler - A container that uses information from the client and preprocessors to produce renderings to be shown to the user.
Service - A reusable running software accessible through network requests to perform special-purpose but common tasks for preprocessors and handlers. For example, text-to-speech. Note that this is separate from a Docker service, although many containers of an IMAGE service may form a Docker service.

///

Introduction
This guide is dedicated to support IMAGE in multiple languages.
Pre-requisite: Knowledge on HuggingFace transformer API & PyTorch, HTTP request/response using JavaScript via fetch-then API and Flask (Python), TypeScript syntax. Knowledge on Regexp is optional but much appreciated.
Server-side components: Translation Service, Handlers, French TTS
Client-side components: UI localisation, Automatic mode
1. Translation Service: multilang-support
TL;DR: The translation service uses large language models from the Helsinki-NLP collection, all based on the Marian training framework. We then use HuggingFace transformers API with PyTorch to activate the model for translation.
The detailed documentation about service can be read here: README.md.
The service takes a request with the following body:
segments: (required) list/array of strings to be translated
src_lang: (optional) the source language. By default it's English 'en'
tgt_lang: (required) the target language
The service sends a response with the following body:
translations: a list/array of strings translated to the target langugage
src_lang, 'tgt_lang`: source and target languages, respectively. This is mostly for debugging purposes.
2. Handlers
Most language handling is located within the condition:
if (targetLanguage !== 'en') {
  // language handling in here
}
"Language handling" may refer to the following actions:
Translate a word, part of a sentence, or a full sentence generated by handlers. Individual string is often referred to as "segment".
Translate the description (also known as the title) of an interpretation.
Common workflow:
Use fetch API to get the translation from multilang-support.
Then, send the translation to espnet-tts-xx where xx is the target language.
Depending on the situation where multilang-support might not support the target language, or there is no TTS service available for such language, we simply skip the step and log the error.

As August of 2023, multilang support is in three handlers: photo-audio-handler, autour-handler, and high-charts handler.
2.1. photo-audio-handler and high-charts
Both handlers have an utils.ts file for our intermediate function to fetch the translation. Their implementations are basically the same:
/**
 * Get translation from multilang-support service
 * @param inputSegment: array of segments to be translated
 * @param targetLang: target language, in ISO 639-1 format
 * @returns array of translated segments, correspond to inputSegment
*/
export async function getTranslationSegments(inputSegment: string[], targetLang: string) {
    return fetch("http://multilang-support/service/translate", {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
        },
        body: JSON.stringify({
            segments: inputSegment,
            src_lang: 'en',
            tgt_lang: targetLang
        }),
    }).then((resp) => resp.json())
    .then(json => json['translations']);
}
The translation is done by taking ttsData "value" field, send it to multilang-support along with targetLanguage (taken from language field of the request). The results is then remapped to ttsData[i]["value"] using a for loop. This also applies for pie-charts of high-charts, and map interpretation of autour-handler.
const translatedValues = await utils.getTranslationSegments(
    ttsData.map((x) => x["value"]),
    targetLanguage
);
for (let i = 0; i < ttsData.length; i++) {
    ttsData[i]["value"] = translatedValues[i];
}
For TTS, the function getTTS(..) in utils.ts takes a list/array of strings with the targetLanguage to get a ttsResponse object via fetch API. This function hides the language handling in server.ts. A small addition to getTTS function to redirect handler to TTS service based on the target language.
// inside getTTS(...) function
if (targetLanguage === "en") {
    ttsUrl = "http://espnet-tts/service/tts/segments";
} else if (targetLanguage === "fr") {
    ttsUrl = "http://espnet-tts-fr/service/tts/segments";
}
else {
   // unable to generate speech due to unavailable TTS service
}
Depending on the type of HighCharts, we process them differently.

Line chart: the interpretation is a single sentence; therefore, we send the graphInfo directly to multilang using [what I like to call] one-line translation for both the interpretation and the description (translation title):
graphInfo = (await utils.getTranslationSegments([graphInfo], targetLanguage))[0];
description = (await utils.getTranslationSegments([description], targetLanguage))[0];
Pie chart: Similar language handling steps as photo-audio-handler and autour-handler, mentioned above
2.2. autour-handler
Without having utils.ts like photo-audio-handler, all language handlings are located within the handler.
Based on the target language, we redirect the TTS stage to the correct HTTP address of the service.
For the translation, we send segments (a list of strings describing locations on the map) with targetLanguage to multilang-support. Notice that here we combine the description with the segments into one array to effectively save processing time.
translateSegments.push(description);
translateSegments.push(...segments);
/* *sending `translateSegments` to translate* */

// Mapping description & segments
description = translated[0];
// Replace `segments` with translated segments
for(let i = 1; i < translated.length; i++) {
    segments[i - 1] = translated[i];
}
3. Browser Extension
3.1. UI localisation
All client-visible elements such as texts, labels, or buttons, must be enclosed inside an HTML element with the class _localisation_ (Canadian spelling!). The element's ID should be the message title/name on the locale JSON.
<!-- E.g.: Directly on the HTML -->
<span class="localisation" id="FullRendering"></span>
<option class="localisation" id="automaticLanguage" value="auto"></option>
// E.g.: JavaScript/TypeScript createElement
const aButton = document.createElement("button");
/* adding attributes, eventlistener, etc. */
aButton.classList.add("localisation");
To automatically retrieve locale message from \_locale/fr/messages.json, import queryLocalisation from utils.ts and add it to the end (or near end) of the script. It will find all elements with class localisation in the DOM and match the message's name with the element's ID, and add the message's content to the webpage.

Messages are added to messages.json file inside the language folder. For example, English messages are located in \_locales/en/messages.json. Although it's not explicitly enforced, I suggest applying CamelCase syntax for the element's name. Description field is optional.

{
    "extensionName": {
        "message": "IMAGE Extension",
        "description": "Extension name"
    },
}
NOTE:
To get browser's UI language directly, see: i18n.getUILanguage()
let UILang = browser.i18n.getUILanguage();
let UILangCode = UILang.slice(0, 2);
In the case of high-charts where the JavaScript is injected to the DOM instead of running in the background via browser API, it's temporary using navigator.language to detect display language for buttons. Also, since we can't use getMessage() function of the API, it's the only component that's hardcoded like below:
if (navigator.language === 'fr')
   chartButtonText = "Interpréter ce graphique avec IMAGE";
else
   chartButtonText = "Interpret this chart with IMAGE";
3.2. 'Automatic' mode
This functionality works differently depending on user's operating system (OS) and browser. Note that there are UI language and renderings (IMAGE results) language.
Windows:
UI and Rendering language is based on the browser settings.
Tested on: Vivaldi, Microsoft Edge, Opera
Linux:
Debian-based (Ubuntu, ChromeOS, etc.)
Arch-based:
Vivaldi: UI Language is based on browser settings; Rendering language is based on system's LANGUAGE variable
Chromium, Google Chrome (AUR): both UI and renderings language is based on system's LANGUAGE variable
MacOS/iOS: Work in progress, support coming soon!
4. Text-To-Speech (TTS) Service
As of August 2023, the IMAGE project has English and French TTS engines available.
5. Known Issues
TTS couldn't pronounce numbers
See French TTS espnet-tts-fr struggles with numerical numbers.
Patched by PR Helping French TTS to pronounce numbers using Regex filtering
Language Model produces repetitive segments
See Segment repetition in French renderings.
Note from @notkaramel: I've seen this bug before in some other languages back in the evaluation days, but very minor and edge case. A probable solution could be fine-tune the model, or change the exisitng model's parameters.
HighCharts button on the browser extension is hardcoded to display depending on navigator.language result. This is an exception since the script doesn't have access to the browser API to query localisation messages.

///

The following are guidelines to follow when working on the project. What specifically should be followed depends on if a particular piece was written by you or another party and how it is integrated into an IMAGE component.

Code used as a library
If linked via a package manager (in package.json, requirements.txt, etc.) then there is nothing more to do.
Otherwise, include mention of it in a component-specific README and a link back to the project's page/repository.
Code added as a file but NOT modified
Include mention of it in a component-specific README and link back to the project's page/repository.
Code added as a file AND modified
Keep any authorship/copyright/license notice included in the file.
Add the IMAGE project header and below that indicate that this is a derivative work and where it came from.
Code snippets added to an original file (e.g., from StackOverflow)
Clearly indicate where the section of code came from, ideally with a link.
If snippets from one source make up the bulk of the file, treat the entire file as a derivative work (see above). Otherwise, also follow the instructions for an original file (see below).
Original file using only original code
IMAGE project header


///


The literature survey was done in September 2022 and it highlights the recent work done by several researchers in extracting the relevant data from charts

One of the first papers that used ML on charts was proposed by Savva et al.[1]. The authors had named their model "ReVision" and tried to incorporate ML and image processing techniques to extract data from charts. The authors classified the charts using a Deep Learning based classifier. Depending on the classification, they suggested custom image processing techniques for different categories of charts.

Other researchers tried to expand on ReVision, by making minor modifications to the architecture[3,5-7]. However, the work done in these papers cannot be generalized to a large set of charts.

Hence to tackle this issue, Dan et al. proposed a Computer Vision(CV) inspired model named Chart Decoder[2]. The Chart Decoder combined image processing with OCR for data extraction. Building up on Chart Decoder, Mishra et al.[4] proposed new model called ChartsVi which tried to generalize the system to a larger group of charts. Fig 1 highlights the workflow of the ChartsVi model.

Screen Shot 2022-09-12 at 5 21 29 PM Fig1: ChartVi Architecture[4]
As visible in Fig1, the first step is to classify the chart. Following the classification, we need to apply image processing techniques. The image processing techniques vary based on the type of chart[3].

A good starting point: I believe that it is not possible to create a single end-to-end ML system which can extract data from all the charts available on the internet. Most researchers initially implement a classifier for detecting the type of chart. Depending on the classification, the researchers determine the image processing techniques that can be applied on the chart.

If we plan on expanding IMAGE to the charts domain, the best possible method would be to do a literature survey on the image processing techniques available for chart data extraction. Additionally, before developing the system, we would have to decide on the categories of charts that we are planning to tackle. This is pivotal as these categories would influence the training of the classifier as well as the image processing techniques used in the system.


///

The following are flowcharts (simply) explaining how different audio experiences are rendered in SuperCollider. Note that these are descriptive and are not directly generated from actual code! A timestamp is provided to indicate when these diagrams were last updated.

Photo Audio
Updated 2022-04-28 and shows how the photo-audio-handler produces its audio for photographs.

Flowchart for the responder in SuperCollider. It contains three branches for three types of rendering parts: plain text, objects, and segments.

